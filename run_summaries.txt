-------
Run 0 -
-------
changes: none this is the first run with a random 
uniform distribution


------------------------
Run Summary: 
------------------------

Minimum Loss: 0.4949580729007721
Iteration of Minimum Loss: 197

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-------
Run 1 -
-------
changes: went from a uniform distribution to a random 
normal distribution

------------------------
Run Summary: 
------------------------

Minimum Loss: 0.46677401661872864
Iteration of Minimum Loss: 1837

Notes:
------
It seems to perform better but not by much which kinda
makes sense because even tho a random normal is closer 
to the data's distribution, the lstm and ffn would just 
have to scale the distribution slightly to get to the same 
results but I am going to stick with it going into the future
runs.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-------
Run 2 -
-------
changes: learning rate 
lr_D = 2e^-6 -> 4e^-6
lr_G = 1e^-6 -> 2e^-6
filter_number = 8 -> 16

------------------------
Run Summary: 
------------------------

Minimum Loss: 0.4921630024909973
Iteration of Minimum Loss: 2275

Notes:
------
converges at around 2000 iterations, and does not do much
better than previous runs. In the future, I will go back to 
8 filters and change the learning rate down just a little.
But if I don't see more obvious changes I am going to commit
to this current learning rate.


+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-------
Run 3 -
-------
changes: learning rate
lr_D = 2e^-6 -> 3e^-6
lr_G = 1e^-6 -> 1e^-6
filter_num = 16 -> 8
hidden_dim = 512 -> 256

------------------------
Run Summary: 
------------------------

Minimum Loss: 0.4873821437358856
Iteration of Minimum Loss: 1011
Average Run Summary: 
Minimum Average of 50 Run Loss: 0.5943513906002045
Iteration of Minimum Average of 50 Run Loss: 194

Notes:
-------
Does not seem to preform much better and the average generator loss
seems to find a local minimum at 10000 iterations, but the total generator
loss is different. I honestly am confused as to whats going on with
the loss function. Will continue to improve.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-------
Run 4 -
-------
changes:
no more learning rate changes
max iterations: 20000 -> 30000
hidden_dim: 256 -> 1024

------------------------
Run Summary: 
------------------------

Minimum Loss: 0.4508230984210968
Iteration of Minimum Loss: 2190
Average Run Summary: 
Minimum Average of 50 Run Loss: 0.4828873437643051
Iteration of Minimum Average of 50 Run Loss: 103

Notes:
------
It seems to perform better but still not passing that
0.40 loss rate like I am looking for. I am sleeping now and
think on improvements tommorow. Current ideas are the 
following:

	(1) Increase the size of the data set
	(2) Compare the Generator with the Discriminator,
	    it seems that the discriminator is really bad
	    why tho?

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-------
Run 5 -
-------
changes:
hidden_dim: 1024 -> 512 
even tho 1024 did better trying to reduce 
runtime for experimentation. Using
this run to look at both the discriminator
and the generator. 

__________________________________________________________

Generator Run Summary: 
------------------------

Minimum Loss: 0.4577149450778961
Iteration of Minimum Loss: 3785
Average Run Summary: 
Minimum Average of 50 Run Loss: 0.4857617664337158
Iteration of Minimum Average of 50 Run Loss: 9600
__________________________________________________________

Discriminator Run Summary: 
------------------------

Minimum Loss: 0.9028991460800171
Iteration of Minimum Loss: 19980
Average Run Summary: 
Minimum Average of 50 Run Loss: 0.9380329596996307
Iteration of Minimum Average of 50 Run Loss: 19950

Notes:
------
I can't tell if the discriminator is bad or if it's just
adjusting to the data creation. I will fix the discriminator 
first

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-------
Run 6 -
-------
changes:
To fix the discriminator I first adjusted the way
loss is computed by adding the reak and fake losses together.
Next, I am going to adjust the amount of runs of the discriminator
per each training iteration.
D-steps_per_G_step: 3 -> 5
max_iteration: 30000 -> 20000

__________________________________________________________

Generator Run Summary: 
------------------------

Minimum Loss: 0.4917096793651581
Iteration of Minimum Loss: 191
Average Run Summary: 
Minimum Average of 50 Run Loss: 0.5306933200359345
Iteration of Minimum Average of 50 Run Loss: 9250
__________________________________________________________

Discriminator Run Summary: 
------------------------

Minimum Loss: 1.3120023608207703
Iteration of Minimum Loss: 19749
Average Run Summary: 
Minimum Average of 50 Run Loss: 1.402327642440796
Iteration of Minimum Average of 50 Run Loss: 19750

Notes:
------
The model seems to perform way worse, I need to improve 
the discriminator more. Will try upping the learning rate.
It seems that the discriminator still does not converge 
on a great result yet

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

-------
Run 7 -
-------
changes:
the goal is to make the discriminator learn faster so 
I decided to up the learning rate and bring the discriminator
steps per generator step back down.
D_steps_per_G_step: 5 -> 2
D_lr: 3e-6 -> 1e-5

__________________________________________________________

Generator Run Summary: 
------------------------

Minimum Loss: 0.47253790497779846
Iteration of Minimum Loss: 325
Average Run Summary: 
Minimum Average of 50 Run Loss: 0.5524885231256484
Iteration of Minimum Average of 50 Run Loss: 3650
__________________________________________________________

Discriminator Run Summary: 
------------------------

Minimum Loss: 1.1832745969295502
Iteration of Minimum Loss: 18518
Average Run Summary: 
Minimum Average of 50 Run Loss: 1.3798941826820375
Iteration of Minimum Average of 50 Run Loss: 19150

Notes:
------
I have no clue what to  make of this. This seems to be
overfitting much quicker than before but I don't know 
why changing the lr or the steps per g step would do 
that. The discriminator is still not improving fast
enough and needs to get better.

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
-------
Run 8 -
-------
changes:
I am going to change the discriminator run amount per
generator to hopefully equalize how quickly the generator overfits.
D_steps_per_G_step: 2 -> 5
filter_num: 8 -> 16


-runsum

Notes:
------
Runs take too long

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

-------
Run x -
-------
changes:

parameter x: 20000 -> 30000


-runsum

Notes:
------
template

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
